{
  "type" : [ "h-entry" ],
  "properties" : {
    "name" : [ "GDS Podcast #35: How our Site Reliability Engineers migrated GOV.UK Pay | Government Digital Service Podcast" ],
    "summary" : [ "Wondered how to migrate a 24/7 product to a serverless platform? We chat about initial user research, developing DevOps skills and the benefits of GDS's approach to this type of tech project.\n \n---------\nThe transcript of the episode follows:\nVanessa Schneider:\nHello and welcome to the Government Digital Service podcast. My name is Vanessa Schneider and I am Senior Channels and Community Manager at GDS. Today, I am joined by Jonathan Harden, Senior Site Reliability Engineer, and Kat Stevens, Senior Developer and co-Tech Lead on GOV.UK Pay.\n \nGDS has many products that rely on our expert site reliability engineers and their colleagues to maintain and improve their functionality. Such as GOV.UK Pay - one of GDS’s common platforms that is used by more than 200 organisations across the UK public sector to take and process online payments from service users. Jonathan and Kat recently completed a crucial reliability engineering project to ensure that GOV.UK Pay continues to operate at the highest standards and provide a reliable service for public sector users and their service users. \n \nWe'll hear more about that in a moment, but to start off, can you please introduce yourself to our listeners? Kat, would you mind starting?\n \nKat Stevens: \nHi I'm Kat Stevens, I’m a Senior Developer on GOV.UK Pay. I've been working at GDS since 2017. And before that, I was a developer at start-ups and small companies.\n \nAs a co-Tech Lead on the migration team then, I'm kind of jointly responsible for making sure that our platform is running as it should be. That our team is working well together, that we're working on the right things and that we're, what we're working on is of a high quality, and is delivering value for our users. So it's like balancing that up with software engineering, making sure that you know, that we're being compliant. It's very important for Pay.  Software [laughs] engineering is so broad: there's like security, reliability, performance, all of those things. So yeah, it's kind of thinking about everything and---at a high level.\n \nVanessa Schneider: \nI'm glad somebody's got a high level overview. Thanks, Kat. Jonathan, would you mind introducing yourself too?\n \nJonathan Harden: \nHi, I'm Jonathan Harden, and I am Senior Site Reliability Engineer on GOV.UK Pay. I've previously worked for a major UK mobile network operator, in the movie industry and for one of the UK's highest rated ISPs.\n \nSo all of GOV.UK Pay's services run, have to run somewhere. Being a Site Reliability Engineer means that I'm helping to build the infrastructure on which it runs, ensure that it is operating correctly and that we keep users’ cardholder data safe and help the developers ease their development lifecycle into getting updates and changes out into the world.\n \nVanessa Schneider: \nHmm..exciting work. So you both worked on a site reliability project for GOV.UK Pay. Can you please, for the uninitiated, introduce our listeners to the project that you carried out?\n \nKat Stevens: \nYeah so recently, we finished migrating GOV.UK Pay to run on AWS Fargate. So previously Pay was running its applications on ECS EC2 instances on AWS. That's a lot of acronyms. But it basically means we were maintaining long-lived EC2 instances that were running our applications. And that incurred quite a high maintenance burden for the developers on our team. And we decided that we wanted to move to a serverless platform to kind of reduce that maintenance burden. And after researching a few options, we decided that Fargate was a good fit for Pay, and we spent a few months carefully moving our apps across to the Fargate platform whilst not having any downtime for our users, which is obviously quite important. Like Pay is a 24/7 service, so we wanted to make sure that our end users had no idea that this was happening.\n \nVanessa Schneider: \nJonathan, how did you contribute to this migration?\n \nJonathan Harden: \nSo obviously, I've only been here for three months, so and the project has been going on quite a lot longer than that. But this is the kind of task I've been involved with, uh, several times now in the last few years at different companies. And so when I joined GDS, it was suggested that I join this project on Pay because I'd be able to contribute really quickly and, and help with the kind of the, the long tail of this migration.\n \nSo a-anybody else that's been in an SR- that works in SRE capacity will know that when you do these kind of projects, you have like the bulk of the migration where you move your applications, like your frontend services that users actually see when they go to the website and the backend services that processes transactions. But then you also have a lot of supporting services around that. So you have services like: things that provide monitoring and alerting, infrastructure that provides where, where do these applications get stored when they're not in use and like where do you launch them from. And there was, there was still quite a bit of that to tie up at the end. And the team, it's quite a small team. As a lot of SRE and infrastructure teams do tend to be. And so when I started, I joined that team and I've been helping with the, the, these long tail parts of the migration. Like in a lot of software engineering, the bulk of the work is done very quickly and the long tail takes quite a bit of time. So, so that's the kind of work that I've been helping with in the last few months.\n \nVanessa Schneider: \nGreat. Kat, as co-Tech Lead, what was your involvement in the migration?\n \nKat Stevens: \nLet’s see where to start. So when I joined the Pay Team, which was around October \n2020, we were in the early stages of the, of the project, so we'd made the decision that we needed to migrate and that involved things like analysing, like co-cost benefit things. I-It doesn't sound that exciting, but it was actually quite cool looking at all the different options. So, for example, it meant that we could keep some of our existing infrastructure. We wouldn't have to move our RDS instances for, for example. We could keep our existing security group, subnets - all that kind of glue that holds all the application, like infrastructure together. \n \nThen there was quite a lot of planning of how we would actually do this, how we would roll out the migration application by application. We've got around a dozen microservices that we were going to move one by one. And figuring out what good looked like. How would we know that the migration is successful. How do we know whether to roll back a particular app.\n \nSo for the actual rollout of migrating sort of one application from EC2 to Fargate: we basically did DNS weighting. So we could have both run--versions of the app running alongside each other, and then you can have 5% of the traffic going to new apps, 95% to the old app. And you can gradually switch over that weighting and monitor whether there are any errors, whether like the traffic suddenly dips and things aren't getting through. So that was all part of the plannings. Like what, what stages would we reach to say like, that yes, we're confident that this change has been positive. And like having a whole, like overview view of what's happening when. Estimating things as well - that's alway, always pretty, [laughs] pretty difficult. But we, as the more apps we did, the quicker we went and we sped up on that. So that was good.\n \nAnd yeah, there's a whole bunch of other things we, we had to get involved with over the last few months as well. So that's things like performance-testing the whole environment to, you know, we wanted to have confidence that the new platform would be able to handle like the high levels of traffic that we see on GOV.UK Pay. Also we wanted to look at how we would actually deploy these apps. Having more confidence in our deployments, moving to continuous deployment where possible. So while those things weren't like directly impacted by Fargate, doing this migration like gave us the opportunity to explore some of those other improvements that we could make. And yeah, I think we've really benefited.\n \nVanessa Schneider: \nThat makes sense, it's always nice to not just keep things ticking over, but making big improvements, that feels really rewarding, I think. Can you give us an impression of what the situation was before the migration maybe?\n \nKat Stevens: \nOn our previous infrastructure, we were running ECS tasks on EC2 launch types - so those are sort of, relatively long-lived instances that we had to provision, patch, maintain. And the developers on the, on the rest of the team, and I--we're not necessarily infrastructure specialists, but when developers on our support rota would end up spending sort of like maybe 5, 6, 7 hours a week just maintaining our EC2 instances, we kind of realised that something had to change [laughs]. And use it, moving to a serverless infrastructure, it's just completely removes that burden of having to provision and make, roll our AMIs, our machine images. We, that just doesn't happen anymore. And we've freed up our developers to work on features. And yeah, the, the infrastructure burden on Pay is just so much less.\n \nVanessa Schneider: \nOh, that sounds really helpful. I’m not sure if migrations are an every-day kind of job for site reliability engineers or software developers, so I was wondering if there’s anything that stood out about this process, like an opportunity to use new tools, or a different way of working?\n \nJonathan Harden: \nSo yeah, it's fun to work with new tools. But there, there, you get to--part of working here, and something I've seen in the time I've been here already, is that we don't rush into those decisions. So it's perfectly possible to see the, the new hot thing in the industry and rush straight for that without a good understanding of what are the trade-offs here. Everything has some trade-offs. And here at GDS, what I've found personally is that we put a lot of effort into understanding what, what's involved in the change; what will the experience be like for - I mean, the customer experience, the user experience, people actually paying for services, that needs to remain rock solid the whole time - but what's the, what's the experience like for developers? So developers have a cycle. They, you know, they write code, they want to test that code somewhere, they want to get it approved and push it to production. And, and so right now, we're undergoing a process of replacing some of our deployment pipelines. And as part of that, we're, we're in the early stages of this, but we're doing real research into how will our change of that be for the developers. And there's something really, really, really rewarding about looking at the different options available, seeing what is the new, the newest cool things, are they where to go? Do you want to go to something a bit, a bit older and a bit more stable? Is there a happy medium? What will the experience for developers be like there? What will the maintenance burden be like?\n \nAnd one of the things for me here is that I'm seeing that e-even down in the teams, it's, these decisions aren't being taken by somebody higher up saying: 'we're going to move to this thing, make it happen'. And instead we've, we're doing research down in the teams that are going to do the work, speaking to the developer-- we're going to be speaking to the developers and surveying all the developers about what do you want from not just the change to stay the same, but change to make an improvement. And it's really, it's exciting to work with the new tools and the new possibilities, and it's also exciting to be involved in making those decisions.\n \nIt marks quite, it was quite stark for me when I first started and I was told this, this major project is going on and it's likely to be 3 to 6 months before we start work, start work on doing it because we're doing the research up front and it's happening in the teams. People are spiking on cool things. Which means even if it's technology that you don't get to use eventually, or that you choose - not don't get to, but choose not to use eventually, you know, the teams are helping to make this choice. You get to try out a bunch of different technologies. And one of the great things with that at GDS is: there are different parts of GDS, and different parts of GDS are using the tooling that is suitable for their area, that makes their area best, work best. And that does mean that there's scope for if you decide I want to work on this other cool thing and this other team are working on it, you can move into one of the other teams and work on that new cool technology.\n \nKat Stevens: \nI mean, I-I-I agree totally. I mean, one of the reasons I wanted to move to Pay was to get more experience working on the infrastructure side of things. On a previous teams it was more sort of stuff like cool software engineering. And on Pay, I've learnt more Terraform than I [laughs] ever thought was possible to know. And loads of other skills like: I've got so familiar with like all the, the intricacies of it as well. And kind of like sort of pushing it to its limits almost, and trying to get the best out of the tools for our, for our team and for our projects. And yeah, it's, it's, it's been really exciting. I mean, one of the new shiny tools that we've been looking at was cloud watch, and we use it for running our smoke tests now. And that was part of the, we kind of like rolled that into the, the Fargate migration project because it seems like a good way of us, like checking that our deployments were working correctly. It took a little bit of wrangling for it to get, fit that into our deployment pipeline. But, but it is really cool sort of like seeing the new thing just falling into place. And now it looks like some of the other teams are following us and using that, that tool as well. So it feels, it feels [laughs] quite nice to be a trailblazer.\n \nVanessa Schneider: \nNo pressure to get it right then [laughs]. What were some of the things on your mind when you were making those selections then?\n \nKat Stevens: \nWe wanted to make sure that we'd made the right decision. So we did spend a fair amount of time actually analysing all the options. In the end, we, we went with Fargate, purely because it meant that we could reuse some of our existing infrastructure.\n \nOverall we kind of prioritised what was going to be the lowest risk in terms of how we were going to do the migration. Like would any sort of mi--you know, would we need any downtime; would this impact like our, our paying users; would it impact on like our service teams, the actual sort of government departments who use Pay; would it im-impact other developers who were actually trying to build new features. And if they've got a platform that's shifting underneath them, that's always going to be difficult. So we were really trying to go for an option that met our needs and like achieved our goals of reducing maintenance burden, saving costs as well, obviously. And yeah, [laughs] just making it, making like Pay an easy, you know, simpler and easier to be a developer on. And weighing that up with, you know, what, what's this like you know, new and shiny thing, like what's all this. Like you know, because there's so many tools out there. But if it's going to take us like a huge amount of effort to actually migrate to them, then I--is that benefit actually going to pay for itself or not? So we, we actually did quite a lot of the investigation analysis, a big spreadsheet [laughs] trying to calculate how much like developer time like in hours per week of what's being spent on infrastructure maintenance and kind of trying to estimate what-- how that would change when we moved.\n \nVanessa Schneider: \nCool, that sounds like the bigger picture view the co-Tech Lead would have of course. Jonathan, any, any benefits that stood out to you perhaps?\n \nJonathan Harden: \nThe, the process of trying these things is really interesting. One of the things that we do at GDS that is not something I've ever experienced elsewhere, I know it does happen elsewhere in the industry, but is, we have what I call firebreaks. So they're a gap between quarters. Now when I say quarters, we're not like planning so these 12 things will happen in the quarter. We are, like our team is running a full Kanban approach because we're an infrastructure team that do some support. And one of the things with those firebreaks is they're a week long. So I've worked lots of places where you do hack days and hack days are great but one day isn't really very much time to truly try something deeply. On the firebreak, you get the opportunity to work, to try something that might-- you know something's coming up. You know you're going to do this migration. You've got some thoughts about, 'ooh, there's this technology. I've heard it's great. I can give it a real try and I can prove to other people that this is something we should seriously consider, especially if it's really exciting for you'. Or you might use the opportunity as well to, to scratch an itch that's been bugging you.\n \nSo like I-I- just to give you an example of what: we've just had a firebreak. And during that firebreak, we saw several different versions of Terraform. For people that know Terraform, some of them were the versions that use the older version of the language - so HTL1 - and some of them with the version that used HTL2, and it means they're not very compatible. So I used that firebreak as an opportunity to upgrade all of our Terraform to get everything up to the very latest. And like that's really scratched an itch for me. And that's not necessarily super exciting for everybody, but for people that have to work on this day to day, it is very, very, very [laughs] exciting. And, but other people did spikes on trying out a whole deploy-- new type of deployment, which is part of what we're doing going forward. And I'm seeing across the other teams, the developer teams, people trying spikes from potential product features, it's very exciting to see those things happening in other teams and people really trying out, and not just a quick hack, but like really trying: 'can we get somewhere with this, and what's the opportunity for using this in the future?' And it's what people wanted to work on. And that's really, really, that was really exciting for me as, as a part of the research, like the ongoing research, the fact that they happen every quarter. It's very exciting.\n \nVanessa Schneider: \nKat, firebreaks - what’s your opinion, are you a fan?\n \nKat Stevens:\nObviously at GDS like our quarters like, you know, we do carry over work between quarters, but it is nice to have that, that week or so where you can just like think about something else. You can, it's, you can recharge, you can reset little bit, you can try something new. And having like the, like the support from senior management to do that as well and have that space to experiment and try out new things to fail as well, I think that's so important. And even if your product like, never makes it outside a firebreak, you can, it will stick in your memory. And so when 6 months later they say, 'oh, maybe we should try this' and you can actually say: 'that might be a disaster. I remember it from my firebreak' [laughs]. Or you've got that background knowledge to just give context on a wider discussion, perhaps. I think it's so useful. \n \nAnd also it kind of gives you an opportunity to potentially collaborate with people who y-you don't normally work with or with people in different roles as well. So rather than just us working within the migration team or the feature teams, we can kind of chop and change. You can work with like User Researchers or Content Designers and do just the things you wouldn't normally do. And or even if you just need a little bit of time to do some housekeeping or tidy ups and stuff that's, like Jonathan said, is just scratching that itch. So I love, I love a firebreak.\n \nVanessa Schneider: \nIt sounds like the firebreaks have been really productive then - are there any other wins you can share from the migration as well perhaps?\n \nJonathan Harden: \nOne of the interesting things, for me one of the interesting things about working in Pay specifically in GDS, is that we have to maintain PCI compliance because we're taking payments. Now that's not something I'd ever done before coming into Pay. So the, the first thing I did in Pay was learn about PCI and spend some time learning about what it, what it means to be compliant. But part of that is called protective monitoring. So you have active scanning going on looking for 'is anything nefarious happening over here, has anything goes wrong over there'. And that means that you, people have to spend time responding to those reports. And those reports, you occasionally get a false positive. But spending all that time dealing with those reports and investigating them like that's, that's all been freed up now.\n \nBut that means we can focus on future improvements more. So we've, our, we have a new environment to test performance of the application in. W e're going through a process at the moment of making it so that that environment can appear when it needs to appear and go away when it doesn't need to be there. And that, of course means saving money, which you know, we work in the Civil Service, this is taxpayer money. This isn't like venture capital, it's the money that all of us pay in tax. And so it's like even more important to make sure that we're spending the right money. It's not to not spend money, it's to spend the right money and only the money that you need to spend. And so we're able to spend time making sure that we can have that environment scale itself down and scale itself back up and use that learning of scaling up and down those environments to start working on potentially auto-scaling the other environments so that they respond to meet demand instead of needing to be at the capacity for peak demand all the time.\n \nThis is, the-these are quite exciting things in themselves, but like we wouldn't have, we wouldn't necessarily have the time to do these smaller improvements that, you know, that will save money. They'll make a big difference in how much we spend.\n \nVanessa Schneider:\nWhat about you Kat, any thoughts?\n \nKat Stevens: \nYeah, so previously while the majority of our apps were running as tasks on EC2 instances, we did have a couple of Fargate apps running. And people were a bit nervous about updating them and deploying them. But now we are deploying to Fargate everywhere, suddenly, it doesn't seem so much of a big deal anymore. And so we've been able to kind of demystify some of those extra auxiliary apps. We've had really good feedback from the developer team saying like: 'this is great. We don't even have to, you know have like a, mental energy spent on worrying about this app anymore'. And that's kind of like the same for our other sort of, the, the bits and pieces that go under the radar. So this is something we're kind of looking at now is: how do we make sure our NginX proxies are patched and up to date and get deployed quickly, and it's not going to be a, a huge mental effort even [laughs] to kind of even think about how do we do this: 'we don't do this very often. Am I going to have to look this up again?' We can automate more of these processes and just have a more stable and reliable platform.\n \nVanessa Schneider: \nIt can be intimidating when you don’t do a process frequently, just wanting to make sure you get everything exactly right, I think a lot of people can relate to that, but it’s so good [laughs] everyone’s confident now!\n \nKat Stevens: \nBig factor but yes.\n \nVanessa Schneider: \nSo, obviously, Kat, you aren't a Site Reliability Engineer, but working on this project has given you the opportunity to upskill in the area. Is that right? Is that a common practise? Is it, is it normal for Software Developers to sort of take on a project like this to learn these things? \n \nKat Stevens: \nIt's interesting. I think the role of a Software Developer at GDS, it can be so broad. And there's so many different types of things you can work on. I was working on Python projects for a couple of years. And I've sort of like, dipping my toes into a bit of Ruby and bit of JavaScript. And...but, but the previous team I was working on, the infrastructure was very stable and there, there wasn't really any, a huge need to like revamp it or do any major bits of work on it. So while there was a couple of bits and pieces ad-hoc here and there, it kind of felt like the, the infrastructure side of the whole software engineering ecosystem, [laughs] for want of a better word, the, the, the infrastructure side of it was, was a gap in my knowledge. And so it's been really good to be able to move to Pay and like roll up my sleeves and get stuck in and you know like, figure out all these IAM permissions, what, what needs to be done where and actually sort of like get, getting that experience in like lifting the hood and seeing what's powering the, the actual software underneath. And almost like going down through the layers and yeah, [laughs] it's been, it's been really eye-opening actually. Like...previously, I would have never described myself as doing any sort of DevOps side of things, and I was actually quite like scared of Bash scripts. And now they are, yeah, well, I wouldn't say second nature, but they're not so scary anymore [laughs].\n \nVanessa Schneider: \nThat's a great outcome in my books. Jonathan, is it common practise to have somebody come in like that for you? I mean, obviously you've not been at GDS for a long time, but I was just wondering how this compares to the private sector.\n \nJonathan Harden: \nSo lots of people want to be a Site Reliability Engineer, it's a very kind of hot field. It's a very cool area to work in. And I don't just mean across the industry. I mean, I think that's a, I really, really like this role. I've put on many hats over my career and this is the one I'm enjoying the most by a long way. But, so in a previous company, I was like leading a team of infras-- there we were calling ourselves Infrastructure Engineers, but we were hiring Site Reliability Engineers. And actually, we found that it, it was, in some ways it was better to have a more diverse team in previous role as well. I mean, like, I always believe it's better to have a diverse team anyway in all aspects. But having people from a software engineering background and people from a systems administration background, like a traditional SysAdmin background, bringing those people together, especially if you've got one or two experienced Site Reliability Engineers already, works really, really well. People want to upskill into this area. Upskill isn't even necessarily the right word. People want to move into this area. It's not that it's an upskill, it's, it's, it's sideways. It's a different kind of role. And it means that they're very enthusiastic and they really want to learn these things and they want to demystify the scary things like Kat was talking about. So me personally, I've been, she mentioned Bash, I've been using Bash for many, many, many years [laughs] since about 2001, I think something like that. So that's not scary for me, but for people who haven't worked with it, I can help them with, like you know, I can help people and I can mentor them and I can show them good practises are.\n \nVanessa Schneider: \nI don't think I've heard a better recommendation for folks to become site reliability engineers - keep an eye out on our vacancies as there are continuously opportunities at GDS to work on exciting projects like this migration, or broaden your skill sets. But just to recap, would you say there’s anything you’re particularly proud of as a result of this migration?\n \nKat Stevens: \nThe--like the actual how we did the rollout itself like with zero downtime. I thought that was pretty cool. But also maybe kind of like in the ways that we actually worked as a team around it as well because it was quite a long running project. And I think there's some interesting parts about how we like re-reassured ourselves that we were doing the right thing. Like, you know, regular retrospectives, firebreaks like we've mentioned, like how we dealt with unexpected work coming along because [laughs] as well as being like the migration team, we are also kind of the infrastructure team. So any kind of unexpected bits and pieces that came up, it would be our team that, we would have to like temporarily pause the migration work and pick up you know, whatever it was. So how we responded to that and you know how we communicated with each other, I-I think that's kind of a whole, a whole other podcast in itself almost.\n \nVanessa Schneider: \nIt sounds like there is an amazing community that you can tap into to keep up to date, make sure that work isn't being duplicated. And clearly there’s a lot to be proud of regarding the product performance. \n \nJonathan Harden: \nYeah, so something that I found a little different here from other places I've worked, even larger organisations, that actually really helps with the sharing of information: so we, we have various like show and tell type catch-up meetings but for a wider than just your small area of the, of the business. So we have a catch up every week amongst all the infrastructure people. And there we all talk about what are we working on right now; like what things are we looking at in the future; are there challenges that you faced; how is the business as usual stuff going in your area. And conversations often come out of that into: 'oh, you're trying out this new technology?' Or you might, because we have it every week, you might mention like: 'oh we're starting to look at this thing' and you'll hear other people's opinions on either the thing you're trying or what you're aiming at or what they've done.\n \nSo we, I was mentioning we're doing this tuning our deployment pipelines, so we have a-a few peo-teams are all doing that as well. And so we have a channel where we're talking about that. And as people are trying things, they're putting in that channel like what they're trying, how it's going, like what the challenges they faced are and, you know, asking for help as well: have other people tried this; what, did you manage to solve this issue or that issue. And I really feel like the collaboration across parts of GDS and the wider Cabinet Office is, is really, really good. within the infrastructure side, it's really good. There's definitely like beyond the infrastructure I do attend, we do have show and tells where people get to show like the thing they're working on that's not just infrastructure related, and that's been, that's really good as well for just understanding like the wider landscape of what's happening across Cabinet Office. And that's that's really, they're really helpful to communicate those things and to work out: 'are we working on the same thing'; 'are you about to start working on the thing that I'm working on'; 'have you already done this and can you give me some pointers'. And that's really good.\n \nVanessa Schneider: \nYeah, it’s nice that you've had the opportunity to share your learnings with the community. Do you have any, maybe, more personal reflections on this work perhaps?\n \nJonathan Harden: \nYeah. So working at the Cabinet Office, it's the first time I've worked for the Civil Service and I'm very aware it's, it's different than the other roles that I've had because I'm like, I feel like I'm kind of helping wider society. We all have to pay the government for all sorts of things. And Pay supports many different services, including - on a previous version of the GDS podcast, you talk to some of the product people from Pay, and I listened to that before I joined Pay, before I joined GDS, and it was really interesting to hear the esoteric services that we have - but of course we have some, we have some bigger services as well and other government departments coming online all the time. And knowing that the infrastructure we're working on supports the ability for the public to pay things that they need to pay to the government or they want to pay, you know, they, like you said, they might be buying a fishing licence or something like that. And that's, knowing that we make it easier for people to do that and that it's done in a way that focuses on the accessibility of the service so any member of the public can try and pay through us and will have, not reach barriers like their screen reader software can't work with the service. \n \nThese are, knowing that I'm giving this back as part of my role, it makes a big difference to me as an Engineer. It's, it's, it's kind of the first, one of the first times where I've not have some kind of crisis around like, 'oh, am I giving back to society, wider society?'. And now I really feel like I am. And that's a real big part of what's making me so happy here among working on a fantastic team and a great org, and on cool technology, of course.\n \nVanessa Schneider: \nThat's so lovely to hear, Jonathan, [laughs] thank you for sharing. If you are similarly minded and want to try and help wider society, do keep an eye on our careers page. That's: GDS careers dot gov dot uk [gdscareer.gov.uk] for openings. It could be in site reliability engineering, it could be general software developer, it could be very different, but we're always looking for new folks to join us and bring their perspective into the organisation. \n \nThank you to Jonathan and Kat for joining me on the episode. If you like it, you can listen to all other episodes of the Government Digital Service Podcast, like Jonathan has, on Spotify, Apple Podcasts and all other major podcast platforms, and the transcripts are also available on PodBean. \n \nGoodbye.\n \nJonathan Harden: \nToodelo.\n \nKat Stevens: \nGoodbye!" ],
    "featured" : [ "https://d2bwo9zemjwxh5.cloudfront.net/ep-logo/pbblog3531320/Ep_356r4v7_1200x628.jpg?s=4ee4b1958346380b53b58657a3da3790&e=png" ],
    "author" : [ {
      "type" : [ "h-card" ],
      "properties" : {
        "name" : [ "PodBean Development" ]
      }
    } ],
    "url" : [ "https://governmentdigitalservice.podbean.com/e/government-digital-service-podcast-35-how-our-site-reliability-engineers-migrated-govuk-pay/" ]
  }
}
